calcNodes <- model$getDependencies(node)
},
run = function() {
currentValue <- model[[node]]
proposalValue <- rnorm(1, currentValue, proposalSD)
model[[node]] <<- proposalValue
logAcceptanceProbability <- calculateDiff(model, calcNodes)
if(runif(1,0,1) < exp(logAcceptanceProbability)) accept <- TRUE else accept <- FALSE
if(!accept) {
model[[node]] <<- currentValue
calculate(model, calcNodes)
}
return()
returnType(void())
})
## Now let's use it for x
## I am deliberately using a proposal standard deviation (proposalSD) that will yield poor mixing
simpleMHx <- simpleMH(simpleModel, 'x', proposalSD = 0.1)
simpleMHx$run()
simpleMHx$run()
xPosterior <- numeric(100)
uncompiledTime <- system.time(
for(i in 1:100) {
simpleMHx$run()
xPosterior[i] <- simpleModel$x
}
)
plot(xPosterior, type = 'b')
simpleMHx2 <- simpleMH(simpleModel, 'x', proposalSD = 0.8)
xPosterior2 <- numeric(100)
uncompiledTime2 <- system.time(
for(i in 1:100) {
simpleMHx2$run()
xPosterior2[i] <- simpleModel$x
}
)
plot(xPosterior2, type = 'b')
uncompiledTime2
simpleMHbetter <- nimbleFunction(
setup = function(model, node, proposalSD) {
calcNodes <- model$getDependencies(node)
savedValues <- modelValues(model, 1)
nimCopy(from = model, to = savedValues, row = 1, logProb = TRUE)
},
run = function() {
currentValue <- model[[node]]
proposalValue <- rnorm(1, currentValue, proposalSD)
model[[node]] <<- proposalValue
logAcceptanceProbability <- calculateDiff(model, calcNodes)
accept <- decide(logAcceptanceProbability)
if(!accept) {
nimCopy(from = savedValues, nodes = calcNodes, to = model, row = 1, logProb = TRUE)
}
return()
returnType(void())
})
simpleMHx_better <- simpleMH(simpleModel, 'x', proposalSD = 0.8)
xPosterior3 <- numeric(100)
uncompiledTime3 <- system.time(
for(i in 1:100) {
simpleMHx_better$run()
xPosterior3[i] <- simpleModel$x
}
)
plot(xPosterior3, type = 'b')
cSimpleModel <- compileNimble(simpleModel)
cSimpleMHx_better <- compileNimble(simpleMHx_better, project = simpleModel)
pumpCode <- nimbleCode({
for (i in 1:N){
theta[i] ~ dgamma(alpha,beta)
lambda[i] <- theta[i]*t[i]
x[i] ~ dpois(lambda[i])
}
alpha ~ dexp(1.0)
beta ~ dgamma(0.1,1.0)
})
## NIMBLE makes a distinction between constants and data
## that other packages don't.  Constants cannot be changed
## after the model is defined by the nimbleModel function.
## Data are values for nodes in a model that will be marked as data.
## Algorithms may  need to know which nodes are marked as data, but
## the actual values may be changed.
## N must be provided as a constant, but t could be provided
## either as a constant or as data (or neither).
pumpConsts <- list(N = 10,
t = c(94.3, 15.7, 62.9, 126, 5.24,
31.4, 1.05, 1.05, 2.1, 10.5))
pumpData <- list(x = c(5, 1, 5, 14, 3, 19, 1, 1, 4, 22))
## Initial values are not necessary but handy to provide now
pumpInits <- list(alpha = 1, beta = 1,
theta = rep(0.1, pumpConsts$N))
## Create the model object.
## The only arguments that must be provided now are code and constants.
## Others can be provided later.
pump <- nimbleModel(code = pumpCode, name = 'pump', constants = pumpConsts,
data = pumpData, inits = pumpInits)
## We refer to pump informally as the "R model" or "uncompiled model".
## Look at what we can do with the model object
## We can access and change values
pump$alpha
pump$alpha <- 1.5
pump$alpha
pump$beta
pump$x
pump$lambda
pump$theta
## Note that there is no pump$t since it was provided as constants, so the following won't work:
pump$t
## User-friendly error messages, anyone?
## verify that lambda[i] == theta[i] * t[i]
pump$theta * pumpConsts$t
## We can also access values using double-bracket notation
pump[['alpha']]
## Just as for R lists, this is useful because we can have a variable with the name
theNodeIwant <- 'alpha'
pump[[theNodeIwant]]
## We can operate the model using calculate, simulate, getLogProb, or calcuateDiff
## calculate returns the log probability of a stochastic variable
calculate(pump, 'x[1]') ## this is the log poisson probability of x[1]
## verify the result
dpois(pump$x[1], pump$lambda[1], log = TRUE)
## For a deterministic variable, calculate updates the value and returns 0
## look at current values of
pump$theta[2]
pump$lambda[2]
## change value of lambda[2] and recalculate value of theta
pump$theta[2] <- 0.5
calculate(pump, 'lambda[2]')
pump$lambda[2]
## verify the result
pump$theta[2] * pumpConsts$t[2]
## getLogProb obtains the most recently calculated value of any log probability
calculate(pump, 'x[2]') ## this actually calculates the value and returns it
getLogProb(pump, 'x[2]') ## this just returns the most recent value, without re-calculating it
## calculateDiff calculates the log probability value and returns the difference between it and the previous value
## this is useful for MCMC and some weighting algorithms
previousLogProb <- getLogProb(pump, 'x[2]')
pump$x[2] <- pump$x[2] + 1
calculateDiff(pump, 'x[2]')
## verify the result
getLogProb(pump, 'x[2]') - previousLogProb
## simulate generates new random values into stochastic nodes
pump$theta[3]
simulate(pump, 'theta[3]')
pump$theta[3]
## note that the log probability is not updated until we call calculate
calculate(pump, 'theta[3]')
## For a deterministic node, simulate is like calculate (but doesn't return anything)
pump$lambda[3] ## This has not yet been updated since change the value of theat[3]
simulate(pump, 'lambda[3]')  ## We could use calculate instead
pump$lambda[3] ## now it is updated
## verify the result
pump$theta[3] * pumpConsts$t[3]
## simulate will not over-ride data values unless we explicitly tell it to
saved_x <- pump$x
pump$x[4]
simulate(pump, 'x[4]')
pump$x[4] ## nothing changed
## use includedData = TRUE to say nodes marked as data should be included in any simulation
simulate(pump, 'x[4]', includeData = TRUE)
pump$x[4] ## now it was changed
## let's return it to its original value before moving on
pump$x <- saved_x
pump$x[4]
## We can use calculate, simulate, getLogProb, or calculateDiff on groups of nodes.
## When we do so, they will always be called in valid order, which is called
## "topologically sorted" order.
cat(pump$theta[5], pump$lambda[5], pump$x[5], '\n')
simulate(pump, c('theta[5]','lambda[5]', 'x[5]'), includeData = TRUE)
cat(pump$theta[5], pump$lambda[5], pump$x[5], '\n')
## verify that lambda[5] is correct
pump$theta[5] * pumpConsts$t[5]
## Let's demonstrate that the simulation order is always valid.
## Simulating lambda[5] before theta[5] would be incorrect, because
## lambda[5] must use the new value of theta[5].
simulate(pump, c('lambda[5]', 'theta[5]', 'x[5]'), includeData = TRUE)
cat(pump$theta[5], pump$lambda[5], pump$x[5], '\n')
## verify that lambda[5] is correct
pump$theta[5] * pumpConsts$t[5]
## This means the simulation was done in correct order
## If we don't provide a set of nodes for calculate, simulate, getLogProb, or calculateDiff,
## the default is to use the whole model
calculate(pump) ## this is the sum of log probability values for the whole model
## let's reset the data to original value(s)
pump$x <- saved_x
## Another way to use groups of nodes is with standard R notation for index sequences:
pump$theta
pump$lambda
simulate(pump, c('theta[1:5]', 'lambda[1:5]'))
pump$theta
pump$lambda
## The first five values of each of theta and lambda have been changed
## When we use calculate with groups of nodes, each of the log probability values is
## stored, and their sum is returned
calculate(pump, c('theta[1:5]', 'lambda[1:5]', 'x[1:5]'))
## getLobProb returns the sum of stored log probability values
getLogProb(pump, c('theta[1:5]', 'lambda[1:5]', 'x[1:5]'))
## calculateDiff returns the sum of log probability differences
## If we call it now, it will return 0 since we haven't changed any of the node values
calculateDiff(pump, c('theta[1:5]', 'lambda[1:5]', 'x[1:5]'))
## Internally, the log probability values are always stored like this:
pump$logProb_theta
## which can be useful for inspection, but the proper way to access these values in a program
## is via getLogProb.
## At this point we have seen that we can operate models by accessing and modifying their values
## and controlling calculations and simulations.  But so far we have needed to know what nodes
## are in the model, for example when we use names like 'theta[1:5]' above.  What we want is to
## to be able to learn about nodes and how they are related from the model object itself.
## Querying the model
## First we can see what nodes and variables are in the model
pump$getNodeNames()
pump$getVarNames()
## Notice there is a mysterious node called lifted_d1_over_beta
## This was inserted by NIMBLE to calculate 1/beta, which is needed as the scale
## parameter (1/rate) of the gamma distribution for the theta[i]s.  We call such nodes
## "lifted" because they have been pulled out of the declaration for the theta[i]s. The "d" in "d1"
## stands for "double precision" and is just a character to ensure a valid name in some cases.
## Otherwise the name of the node is semi-reading, such as "1_over_beta".
## We can get information about any of the variables:
pump$getVarInfo('x')
## This shows the number of dimensions (nDim), the range for each dimension, from mins (always 1), to maxs.
## In this case we see x has length of 10 because it has 1 dimension and the maximum index is 10.
## We can learn about the roles played by different nodes.
## To learn more about getNodeNames, we can look at its arguments
args(pump$getNodeNames)
## or the help page for the model class
help(modelBaseClass)
## Let's try some of the options
pump$getNodeNames(determOnly = TRUE) ## deterministic nodes
pump$getNodeNames(stochOnly = TRUE) ## stochastic nodes
pump$getNodeNames(stochOnly = TRUE, includeData = FALSE) ## stochastic nodes without data nodes
pump$getNodeNames(dataOnly = TRUE) ## data nodes
pump$getNodeNames(topOnly = TRUE) ## top-level nodes, typically parameters
pump$getNodeNames(latentOnly = TRUE) ## latent nodes (nodes that are not data and are not top nodes)
pump$getNodeNames(latentOnly = TRUE, stochOnly = TRUE) ## stochastic latent nodes
pump$getNodeNames(includeRHSonly = TRUE) ## This has no impact for this example.  If t was not in constants, it would be a "right-hand-side only" node
pump$getNodeNames(endOnly = TRUE) ## For this example, end nodes are only the data nodes, but in other cases they can include posterior predictive nodes
## Every vector of nodes was returned in topologically sorted order.
## This means we can use them in calculate, simulate, getLogProb, and calculateDiff
## Let's get the sum of log probability values for the data nodes:
getLogProb(pump, pump$getNodeNames(dataOnly = TRUE))
## Notice something very important: We did not have to know anything about pump to do that.
## The same call would work no matter what is in the pump model.  This is the first step
## to model-generic programming.
## Next, we can get information about how nodes are connected in the model.
## Say we want to do MCMC on 'theta[5]' and we need to know which nodes depend on it.
## We call these the "stochastic dependencies"
## To keep it generic, we'll put the name 'theta[5]' in a variable
targetNode <- 'theta[5]'
pump$getDependencies(targetNode)
## We see that following a change to 'theta[5]', the nodes that will need
## (re-)calculation are 'theta[5]', 'lambda[5]', and 'x[5]'.
## By default, the dependencies include the targetNode itself, but this can
## be explicitly excluded (see below).
## This allows more model-generic programming.  For example
## if we modify targetNode, we can determine what needs recalculation
## more *any* model we are using:
## Let's suppose for illustration that we are simply going to add 0.1 to the target node
pump[[targetNode]] <- pump[[targetNode]] + 0.1
calculate(pump, pump$getDependencies(targetNode))
## Note that dependencies of one of the top-level parameters can be larger;
pump$getDependencies('alpha')
## We can also get dependencies of groups of nodes
pump$getDependencies('theta[6:10]')
## Now let's see some of the control we have over dependencies
## Again we can learn about the function by
args(pump$getDependencies)
## or
help(modelBaseClass)
## We can see that the arguments to control what we want from
## getDependencies are similar to those for getNodeNames, so
## we won't walk through them all.  But here are a couple of distinct
## ones to note:
## Use self = FALSE to omit the nodes provided to the function
pump$getDependencies(targetNode, self = FALSE)
## Use downstream = TRUE to trace dependencies through the entire graph
## rather than stopping at stochastic nodes
pump$getDependencies('alpha', downstream = TRUE)
## Writing simple programs to use models
## Let's say you are going to need to repeatedly simulate part of your model
## and from each simulation you want the sum of log probabilities returned
##
## If you were going to write a simple R function, it might look like this
simulateSomeNodes <- function(model, nodes) {
dependencies <- model$getDependencies(nodes)
simulate(model, nodes)
calculate(model, dependencies) ## last value in an R function is always returned
}
simulateSomeNodes(pump, 'theta[1:5]')
## That worked, but let's think more deeply about what we need.
## Often, algorithms for general models will be highly computational and may
## run for minutes, hours or days.  So we want to achieve efficiency where we can.
## The "model$getDependencies(nodes)" is redundant if we are going to call simulateSomeNodes
## repeatedly with the same arguments.
## NIMBLE provides a two-stage programming system using nimbleFunctions to do this.
## We can rewrite the above concept this way
simulateSomeNodesNF <- nimbleFunction(
setup = function(model, nodes) {
dependencies <- model$getDependencies(nodes)
},
run = function() {
simulate(model, nodes)
ans <- calculate(model, dependencies)
return(ans)
returnType(double())
}
)
## Notice that the arguments to nimbleFunction include two function definitions,
## one called "setup" and one called "run"
## Now there are two steps to using this function.  Let's look first and then explain:
simulateSomeNodesNF_theta1to5 <- simulateSomeNodesNF(pump, 'theta[1:5]')
simulateSomeNodesNF_theta1to5$run()
## When we call simulateSomeNodesNF, the arguments go to the setup function, which is executed.
## Any objects created -- in this case "dependencies" -- are saved for later use.  The object returned
## simulateSomeNodesNF can be used to call the run function.  It uses the saved "dependencies" rather
## than creating it each time.  Now if we need to call it repeatedly, it will be more efficient
## than if we were determining dependencies each time.
simulateSomeNodesNF_theta1to5$run()
simulateSomeNodesNF_theta1to5$run()
simulateSomeNodesNF_theta1to5$run()
## Something else to notice is that we had to say what type of object will be returned  by the run
## function.  This is given by "returnType(double())".  In this case, "double()" refers to a double precision
## (numeric) scalar.  It is equivalent to say "double(0)", meaning a double with 0 dimensions.
## The return type is used by the compilation system, which we introduce next.
## Compiling the model and nimbleFunctions
## So far we have been using the uncompiled model and nimbleFunctions.  That means whenever we do something,
## the internal steps are being executed completely in R.  For many algorithms, execution in R is (much) too
## slow to make one happy.  So NIMBLE can generate C++ for the model and nimbleFunctions, compile the C++, load
## the compiled stuff, and let you use it in the same was as the uncompiled versions.
## We can compile the model and nimbleFunctions in one step or multiple steps.  Here we'll use one step
compiledPumpStuff <- compileNimble(pump, simulateSomeNodesNF_theta1to5)
cpump <- compiledPumpStuff$pump
csimulateSomeNodesNF_theta1to5 <- compiledPumpStuff$simulateSomeNodesNF_theta1to5
cpump$x
cpump$theta
csimulateSomeNodesNF_theta1to5$run()
pumpMCMCconf$removeSamplers('theta[1:5]')
pumpMCMCconf <- configureMCMC(pump)
pumpMCMCconf$getSamplers()
pumpMCMCconf$removeSamplers('theta[1:5]')
for(targetNode in pump$expandNodeNames('theta[1:5]'))
pumpMCMCconf$addSampler(target = targetNode, type = 'RW')
## Check the complete list
pumpMCMCconf$getSamplers()
## Also by default only top level nodes are recorded (monitored) for output.
## For this model let's add the thetas for monitoring
pumpMCMCconf$addMonitors('theta')
pumpMCMC <- buildMCMC(pumpMCMCconf)
cPumpMCMC <- compileNimble(pumpMCMC, project = pump)
cPumpMCMC$run(1000)
samples <- as.matrix(cPumpMCMC$mvSamples)
colnames(samples)
dim(samples)
plot(samples[,'alpha'], samples[,'beta'])
pumpMCMCconf2 <- configureMCMC(pump)
pumpMCMCconf2$removeSamplers(c('alpha', 'beta'))
pumpMCMCconf2$addSampler(target = c('alpha','beta'), type = 'RW_block')
pumpMCMC2 <- buildMCMC(pumpMCMCconf2)
cPumpMCMC2 <- compileNimble(pumpMCMC2, project = pump, resetFunctions = TRUE) ## we need resetFunctions = TRUE when building 2nd MCMC for the same model
cPumpMCMC2$run(1000)
samples2 <- as.matrix(cPumpMCMC2$mvSamples)
plot(samples2[,'alpha'], samples2[,'beta'])
load("C:/Users/Adam/Documents/UC Berkeley post doc/BIGCB/Pest Project/Occupancy modeling/Gio spatial effects model cluster run/spGLM_workspace.Rdata")
source("C:/Users/Adam/Documents/UC Berkeley post doc/BIGCB/Pest Project/Occupancy modeling/Gio spatial effects model cluster run/spGLM-model.R")
names(spGLM_data)
dat <- spGLM_data$list.mat
dim(dat)
str(dat)
dat[1:10,1:10]
32*3
32 + 96
rm(list = ls())
my_packages<-c('lme4', 'lmerTest', 'data.table', 'tidyr', 'lattice', 'dplyr', 'bbmle', 'optimx')
lapply(my_packages, require, character.only=T)
citation("lme4")
8*75
rm(ls = list())
rm(list = ls())
library(lattice)
library(coda)
library(R2WinBUGS)
library(R2jags)
plantData <- data.frame(strain = c(rep("WT", 19*2),
rep("mutant", 20*2),
rep("mutant.compl", 12*2)),
distance = c(rep(1,19),rep(15,19),
rep(1,20),rep(15,20),
rep(1,12),rep(15,12)),
infected = c(rep(1,17),rep(0,2),rep(1,15),rep(0,4),
rep(1,19),rep(0,1),rep(0,20),
rep(1,9),rep(0,3),rep(1,10),rep(0,2)))
plantData$strain <- factor(plantData$strain, levels(plantData$strain)[c(3,1,2)])
table(plantData$strain, plantData$infected, plantData$distance)
rm(list = ls())
#### Preliminaries
my_packages<-c('lme4', 'lmerTest', 'data.table', 'tidyr', 'lattice', 'dplyr', 'bbmle',
'detect', 'optimx')
lapply(my_packages, require, character.only=T)
## Set working directory and load Occupancy functions
setwd("C:/Users/Adam/Documents/GitHub/potato_psyllid_distribution_modeling")
source("museum_specimen_analysis_functions.R")
# Load species lists data set with climate data
AllLists <- readRDS("abundance_modeling/All_Hemip_Counts_Climate_15km_Cells_2016-02-26.rds")
# Collectors of potato psyllids, from RawRecords data set in making_species_lists
ppCollectors <- readRDS("potato_psyllid_collectors.rds")
AllListsDF <- AllLists %>% rbindlist() %>% as.data.frame()
# lists that contain collectors of potato psyllids
ppcCollections <- AllListsDF[AllListsDF$Collector %in% ppCollectors, "collectionID"]
ppcData <- AllListsDF[AllListsDF$collectionID %in% ppcCollections,]
ppcLists <- ppcData %>% make_lists(., min.list.length = 1)
# Transform to data frame with pp detection
countData <- countDataFunc(ppcLists, "Bactericera.cockerelli")
# Select only long lists (list_length >= 3)
countData <- countData[countData$list_length > 2,]
countData$lnlist_length <- log(countData$list_length)
countData$lntotal_n <- log(countData$total_n)
covars <- c("year", "month", "lnlist_length", "lntotal_n", "diversity", "aet", "cwd", "tmn", "tmx")
covars.i <- as.numeric(sapply(covars, function(x) which(names(countData) == x), simplify = TRUE))
for(i in covars.i){
var.i <- names(countData)[i]
stdname.i <- paste("std", var.i, sep = "")
stdvar.i <- standardize(countData[,var.i])
countData[,stdname.i] <- stdvar.i
}
# Just potato psyllid data points
ppData <- countData[countData$count >= 1,]
# Exploring data
table(countData$count)
countData <- dplyr::filter(countData, !is.na(aet) & !is.na(cwd) & !is.na(tmn) & !is.na(tmx))
cellTable <- countData %>% group_by(cellID) %>% summarise(nvisits = length(cellID))
cellTable
dim(countData)
head(countData)
table(countData$list_length)
names(countData)
source("museum_specimen_analysis_functions.R")
countData <- countDataFunc(ppcLists, "Aphis.gossypii")
countData <- countData[countData$list_length > 2,]
countData$lnlist_length <- log(countData$list_length)
countData$lntotal_n <- log(countData$total_n)
covars <- c("year", "month", "lnlist_length", "lntotal_n", "diversity", "aet", "cwd", "tmn", "tmx")
covars.i <- as.numeric(sapply(covars, function(x) which(names(countData) == x), simplify = TRUE))
for(i in covars.i){
var.i <- names(countData)[i]
stdname.i <- paste("std", var.i, sep = "")
stdvar.i <- standardize(countData[,var.i])
countData[,stdname.i] <- stdvar.i
}
table(countData$count)
countData <- countDataFunc(ppcLists, "Bactericera.cockerelli")
countData <- countData[countData$list_length > 2,]
countData$lnlist_length <- log(countData$list_length)
countData$lntotal_n <- log(countData$total_n)
# standardize numeric covariates, include as new variables in data frame
covars <- c("year", "month", "lnlist_length", "lntotal_n", "diversity", "aet", "cwd", "tmn", "tmx")
covars.i <- as.numeric(sapply(covars, function(x) which(names(countData) == x), simplify = TRUE))
for(i in covars.i){
var.i <- names(countData)[i]
stdname.i <- paste("std", var.i, sep = "")
stdvar.i <- standardize(countData[,var.i])
countData[,stdname.i] <- stdvar.i
}
# Just potato psyllid data points
ppData <- countData[countData$count >= 1,]
# Exploring data
table(countData$count)
names("countData")
names(countData)
countData <- countData[,c("year", "cellID", "month", "list_length", "aet", "cwd", "tmn", "tmx",
"stdlnlist_length", "stdaet", "stdcwd", "stdtmn", "stdtmx")]
countData <- countDataFunc(ppcLists, "Bactericera.cockerelli")
# Select only long lists (list_length >= 3)
countData <- countData[countData$list_length > 2,]
countData$lnlist_length <- log(countData$list_length)
countData$lntotal_n <- log(countData$total_n)
# standardize numeric covariates, include as new variables in data frame
covars <- c("year", "month", "lnlist_length", "lntotal_n", "diversity", "aet", "cwd", "tmn", "tmx")
covars.i <- as.numeric(sapply(covars, function(x) which(names(countData) == x), simplify = TRUE))
for(i in covars.i){
var.i <- names(countData)[i]
stdname.i <- paste("std", var.i, sep = "")
stdvar.i <- standardize(countData[,var.i])
countData[,stdname.i] <- stdvar.i
}
countData <- countData[,c("year", "cellID", "month", "list_length", "aet", "cwd", "tmn", "tmx",
"stdlnlist_length", "stdaet", "stdcwd", "stdtmn", "stdtmx", "count")]
metaData <- "count = count of potato psyllid museum specimens collected in a given collecting event.
year = year collected. cellID = spatial cell. month = month collected.
list_length = total number of insect species collected.
Climate variables: aet = actual evapotransporation; cwd = climate water deficit;
tmn = minimum annual temperature; tmx = maximum annual temperature.
Variables with prefix 'std' are standardized based on mean and SD."
countData2 <- list(metaData, countData)
metaData <- "count = count of potato psyllid museum specimens collected in a given collecting event.
year = year collected. cellID = spatial cell. month = month collected.
list_length = total number of insect species collected.
Climate variables: aet = actual evapotransporation; cwd = climate water deficit;
tmn = minimum annual temperature; tmx = maximum annual temperature.
Variables with prefix 'std' are standardized based on mean and SD.
Data compiled by Adam Zeilinger 2016-04-02."
countData2 <- list(metaData, countData)
countData2[[1]]
metaData <- c("count = count of potato psyllid museum specimens collected in a given collecting event.",
"year = year collected. cellID = spatial cell. month = month collected.",
"list_length = total number of insect species collected.",
"Climate variables: aet = actual evapotransporation; cwd = climate water deficit;",
"tmn = minimum annual temperature; tmx = maximum annual temperature.",
"Variables with prefix 'std' are standardized based on mean and SD.",
"Data compiled by Adam Zeilinger 2016-04-02.")
countData2 <- list(metaData, countData)
countData2[[1]]
saveRDS(countData2, file = "potato_psyllid_count_data_for_GLMM.rds")
